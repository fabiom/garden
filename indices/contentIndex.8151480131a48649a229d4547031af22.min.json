{"/":{"title":"Entrada","content":"\nEste é um jardim matemático dedicado à ideia de [[medidas-de-informacao|informação]].\n\nPesquise pelo jardim usando `Ctrl` + `k`","lastmodified":"2022-07-22T02:58:07.511490062Z","tags":null},"/medidas-de-informacao":{"title":"Medidas de Informação","content":"\nExistem várias definições quantitativas de informação, mas a maioria delas é formulada utilizado teoria de probabilidade. A intuição por trás disso é que informação é algo que reduz incerteza, e podemos descrever incerteza como um conceito probabilístico.\n\n## Tipos\n\nExistem três tipos de medidas de informação. São eles:\n\n1. **Informação entrópica:** fornece a quantidade de informação contida em uma [[variavel-aleatoria|variável aleatória]] ou [[distribuicao-de-probabilidade|distribuição de probabilidade]], medindo a quantidade de incerteza ou de surpresa obtidas no resultado de um experimento.\n\n2. **Informação paramétrica:** mede a informação fornecida sobre um parâmetro desconhecido por um experimento aleatório.\n\n3. **Informação relativa:** expressa a quantidade de informação fornecida pelos dados para discriminar a favor de uma distribuição em detrimento de uma outra. Pode ser vista, assim, como uma medida de dissimilaridade entre distribuições.\n\n## Propriedades\nPodemos listar algumas propriedades desejadas para medidas de informação. No entanto, nem todas são satisfeitas pelas medidas usadas na prática. Sejam $X$ [[variavel-aleatoria|variável aleatória]], e $T(X)$ uma [[funcao-mensuravel|função mensurável]]. Vamos denotar uma medida de informação de qualquer tipo por $I$, e a informação de $X$ por $I_X$. Algumas boas propriedades desejadas de medidas da informação são: \\[1\\]\n\n1. Não-negatividade: $I_X \\ge 0$.\n\n2. Algum tipo de aditividade:\n\t- Forte: $$I_{X,Y} = I_X + I_Y,$$ se $X$ e $Y$ são [[independentes]].\n\t- Fraca: $$I_{X,Y} = I_X + I_{Y|X}.$$\n\t- Subaditividade: $$I_{X,Y} \\le I_X + I_Y.$$\n\t- Superaditividade: $$I_{X,Y} \\ge I_X + I_Y.$$\n\n3. Desigualdade [[probabilidade-condicional|condicional]]: $I_Y \\ge I_{Y|X}.$\n\n4. Máxima informação: $I_X \\ge I_{T(X)}$ (ver [[desigualdade de processamento de dados]]).\n\n5. Invariância por suficiência: $I_{X} = I_{T(X)}$ se, e somente se, $T$ é [[estatistica-suficiente|estatística suficiente]].\n\n6. Convexidade: Se $\\alpha \\in [0,1]$, $f_1,f_2$ são funções [[funcao-densidade|densidade]], e definimos $X_\\alpha \\sim f_\\alpha$, onde $f_\\alpha = \\alpha f_1 + (1-\\alpha)f_2$, então\n$$\nI_{X_\\alpha} \\le \\alpha I_{X_1} + (1-\\alpha)I_{X_2}.\n$$\n\n\n## Medidas de informação usuais\nAlgumas das medidas de informação utilizadas são:\n\n- Informação entrópica:\n\t- [[Entropia de Shannon]]\n\t- [[Entropia de Hartley]]\n\t- [[Entropia de Tsallis]]\n\t- [[Entropia de Rényi]]\n- Informação paramétrica:\n\t- [[Informação de Fisher]]\n- Informação relativa:\n\t- [[Entropia relativa]]\n\t- [[Entropia relativa simetrizada]]\n\t- [[Distância de Hellinger]]\n\t- [[Distância de Fisher-Rao]]\n\t- [[Divergência de Cauchy-Schwarz]]\n\n\n## Referências\n\\[1\\] K. Ferentinos, T. Papaioannou - [*New parametric measures of information*](https://core.ac.uk/download/pdf/82659216.pdf)","lastmodified":"2022-07-22T02:58:07.511490062Z","tags":null}}